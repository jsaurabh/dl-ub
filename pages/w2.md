---
layout: page
title: Improvements
tagline: Getting Better at doing Deep Learning
description: Week2 material w/ code
---

Table of Contents:
1. [Overfitting](#ovftng)
2. [L2 Regularization](#l2)
3. [L1 Regularization](#l1)
4. [Weight Init](#winit)
5. [Further reading](#reading)

Last week, we looked at some fundamental concepts in training deep neural networks. This week, we'll expand on the idea by going deeper into important concepts such as regularization, weight init, adaptive optimizers and if time permits, learning rate schedules. That said, let's get started. 

As usual, all the code can be found [here](https://jsaurabh.github.io/jsaurabh/dl-ub/tree/master)

## Dealing with overfitting <a name="ovftng"></a>

Deep neural networks are incredible flexible in terms of the data they can represent. The question to ask is, are these networks powerful because of the freedom such a large space allows in terms of representing data or are they actually learning some meaningful insights from the data they're trained on? 

In most cases, it is plausible to assume that the network is actually overfitting on the training data rather than learning the inherent distributions and any meaningful patterns contained within the data. 

    Overfitting, in terms of metrics, occurs when the accuracy for the test set is not in line with the train set. Of course, it's much more nuanced than that, but this is the working definition for us. What that means is the network we train is learning the data and noise specific to the train set rather than learning about the distribution space that the training data is drawn from. 

The first approach we'll look at for dealing with overfitting is called *early stopping*. We'll use the validation set as an abstraction layer here. For every loop through our data, we'll look at our validation accuracy and once it stops getting better, we stop training further. In reality, we'll just go back to the state of the network at that instant rather than stop training. This is how Keras implements early stopping, as a callback function that you can add to your model which continuously monitors the accuracy and saves the state(weights, bias and hyperparams) at the epoch.

Next, let's look at regularization, which lets us deal with overfitting on ever-increasing architecture sizes. 

## L2 Regularization <a name="l2"></a>

Also known as Ridge Regularization, L2 will add a regularization term that's the sum of the square of all the weights in the network. This regularization term is scaled by a factor of &lambda;/2n. 

Mathematically, it is represented as ![equation](https://latex.codecogs.com/gif.latex?C%20%3D%20C_0%20&plus;%20%5Cfrac%7B%5Clambda%7D%7B2n%7D%20%5Csum_w%20w%5E2%2C)

Simply put, L2 will force the weights to be small, with the idea being that small weights won't lead to large deviations from the expected path. &lambda; controls what term in the cost function gets more importance and the larger the value of &lambda; the more preference is given to smaller weights. 

From a gradient descent perspective, the learning rate will end up being scaled by a factor of  ![equation](https://latex.codecogs.com/gif.latex?1-%5Cfrac%7B%5Ceta%20%5Clambda%7D%7Bn%7D). This factor is also called as weight decay, and does exactly what you think. The weights are being driven towards zero, controlled by the regularization coefficient &lambda;.

**NOTE**: To go through an example to visualize the effects of L2, click [here](https://github.com/jsaurabh/dl-ub/tree/master) and go to the Colab link. 

### But why and how does L2 actually work?

During backpropagation, the learning rule adds a weight delta value to each weight. This weight delta is basically a fraction of the derivative of the error function. This is all multiplied by the learning rule hyperparameter. If you take the derivative of the regularization term, the exponent 2 and the divide by half-term cancel out leaving the update rule to be &lambda; * *w*. This essentially puts us in control of the penalty we give to the network for larger weigthts. Smaller weights won't change the behaviour of the network too much, instead learning to recognize patterns that are evident throughout the network rather than local noise. 

## L1 Regularization <a name="l1"></a>

L1 regularization is intuitively similar to L2, except now we add the sum of the absolute values of the weights rather than the sum of squares. 

Mathematically, it is represented as ![equation](https://latex.codecogs.com/gif.latex?C%20%3D%20C_0%20&plus;%20%5Cfrac%7B%5Clambda%7D%7Bn%7D%20%5Csum_w%20%7Cw%7C)


In L2, weight decay is proportional to w whereas *weight deacy* in L1 is by a constant amount. For large weights, L1 shrinks the weight much less than L2 and vice versa. This leads to L1 weights being sparse, with a majority of the weights driven to 0 while some of them turn up to 1.

We've covered a lot of ground in the last week or so. Starting with loss functions, activation functions to regularization and overfitting strategies such as dropout, we've come a long way. Let's tackle one final fundamental piece before we move onto the cool stuff. 

## Weight Initialization strategies <a name="winit"></a>



## Reading <a name="reading"></a>

1. [Chapter 3](http://neuralnetworksanddeeplearning.com/chap3.html)
2. [L2 Regularization](https://visualstudiomagazine.com/articles/2017/09/01/neural-network-l2.aspx)