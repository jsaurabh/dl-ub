Table of Contents:
1. [Introduction](#Image-Classification-using-MNIST)
2. [Loss Functions](#Loss-Functions)
3. [Gradient Descent for Learning](#Learning)
4. [Binding everything together](#Binding-it-all-together)
5. [Further reading](#Reading)

Let's start off with a case study. We'll define a network to do some handwritten digits classification aka MNIST. The only catch: we can use only numpy. The idea behind this choice is to go through some of the fundamental concepts of neural networks without using any high level frameworks. It'll be much more easier to translate these ideas to PyTorch/TF2 etc once you've built it from scratch using a tool like numpy.

**NOTE**: If you'd like to skip straight to the code, go [here](). All the code can be found on my [GitHub](https://github.com/jsaurabh/dl-ub)

## Image Classification using MNIST

MNIST is a dataset that contains thousands of scanned images of handwritten digits.

![](http://neuralnetworksanddeeplearning.com/images/digits_separate.png "Sample MNIST Images")

The images are grayscale(1D) and 28x28 pixels in size. The goal of our network will be to generate a 10 dimensional (sparse) vector for each input, and each value can be either 0 or 1. This idea of representing categorical variables as binary vectors is called as one-hot encoding and something that you'll find used extensively while training neural networks.

Let's introduce another abstraction here: the concept of a cost function. Cost functions, also known as loss/objective functions are mathemagic that tell us how good our network is currently doing. To be an effective machine learning practitioner is to have an intuitive understanding on what cost functions do. 

### Loss Functions
For classification, an appropriate loss function would be **cross-entropy**(also called log loss). If you're wondering why that is, come see after but in a nutshell, cross entropy loss allows us to heavily penalize cases where our prediction is wrong. Cross entropy is a non-negative loss function and tends to zero as as the network gets better. For classification purposes, perhaps the most important distinction between say, Mean Squared Error and Cross-Entropy is that learning is faster. To borrow from Michael Nielsen's excellent online book, **the larger the error, the  faster the neuron will learn.** [2]

So far, we've identified what we want to do. We have defined how we'll measure our network on the performance it gets. What we haven't explored so far is how we'll actually get there. What we want is a procedure that lets us approximate y(x) for each input x. This is where **gradient descent** comes into play.

### Learning 
For now, think of classification as a minimization problem(and in some ways, it is one). Given an input and a multi-variable cost function, we want to minimize it. This is the part where the fun starts. 

Detour:
Before we get into it, it's in our interests to think of an analogy. We'll come back later and wrap it with concepts specific to what we're trying to do.

    Consider you're at the top of a mountain, blindfolded and you want to reach the valley below. One way to start would be to determine what the steepest descent is and go down that path. Remember, you're blindfolded so there's no way to look at the real valued input and determine the direction of descent. 

This is what we'll use gradients for. Gradients are the partial derivatives of multi-variable functions. A derivative is the rate of change for a function at any given point. Calculating these gradients should give us an idea about the 'shape' of the valley and help us choose the path we should take to get to the bottom. Then we update the parameters in our multi-variable function and this constitutes one step in the learning process. 

For every step, the cost function changes as the product of the gradient vector and the random movements themselves. This makes sense, as for changes in movements, the gradient vector makes the updates to the cost function. We can define these updates in terms of the learning rate such that the gradient always decreases, thus ensuring that the cost will always go down. For a concrete mathematical representation of these ideas, go [here](/pages/sgd.md).

Coming back, now we've got an algorithm, that for every step(variable update) minimizes the cost function and makes a step(movement) in the opposite direction to 'fall' down the valley. This is exactly what we wanted. This however, brings up an important point: the learning rate is an important parameter that needs to be set accordingly. Too small a rate, and we'll never reach the global minimum fast enough. Too fast(too high) and we'd end up with a non-negative update which is the opposite of what we want. 

**NOTE**: This is a highly abstracted, simplified explanation of gradient descent. There are many potential issues, one of which we'll cover in the next section.

### Binding it all together
I've mentioned multi-variable cost functions without going into what those variables are. For this case study, we'll just consider the weights(w) and biases(b) parameters. In the context of neural networks, the idea of making a movement is reflected by the *weight(w)* and *bias(b)* parameters. We make 'movements' by modifying these parameters to reflect the effect of a possible movement(another abstraction!). The update rule thus becomes 
       
<center> w<sub>i</sub> = w<sub>i</sub> - &eta; * &nabla;C </center>
<center> b<sub>i</sub> = b<sub>i</sub> - &eta; * &nabla;C </center>
where &nabla;C is the gradient and &eta; is the learning rate parameter
 
The gradient descent algorithm we've looked at earlier computes the gradient &nabla;C for each input example x and then averages over the number of inputs. For large datasets, this means learning updates can be very slow. There is a very simple way out of this: random sampling. 

We *stochastically* choose a number of samples(say l), called a mini-batch. As long as the size of a mini batch is large enough such that the average of gradients over a mini-batch is roughly equal to the gradient over the dataset, we're good to go. The update rule changes as follows:

<center> w<sub>i</sub> = w<sub>i</sub> - &eta;/l * &nabla;C<sub>X<sub>i</sub></sub> </center>
<center> b<sub>i</sub> = b<sub>i</sub> - &eta;/l * &nabla;C<sub>X<sub>i</sub></sub> </center>
where &nabla;C<sub>X<sub>i</sub></sub> is the gradient over the training examples for the current mini-batch. 

To recap, the gradients over a mini-batch of training inputs(X<sub>i</sub>) are applied to the entire dataset. Then, we randomly sample another mini-batch and so on. When all training inputs have been used, it constitutes one epoch. Learning is continued for the number of epochs we define.

---
## Reading

1. [CS231N Numpy Tutorial](http://cs231n.github.io/python-numpy-tutorial)
2. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap3.html)